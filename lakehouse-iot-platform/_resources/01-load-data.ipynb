{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77da4442-2159-4efa-8e60-1d93b4ddddfb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install git+https://github.com/QuentinAmbard/mandrova faker databricks-sdk==0.36.0 mlflow==2.17.2 \n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd0f211b-7d63-43af-94d1-ed3db23601d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.dropdown(\"reset_all_data\", \"false\", [\"true\", \"false\"], \"Reset all data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be4f5950-bbc9-461c-838a-aa052f8ba08c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run ../config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7064cea4-21a9-485a-9ff7-222a4928c244",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run ./00-global-setup-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7667326-1d47-4873-9695-dd7087b97d30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "reset_all_data = dbutils.widgets.get(\"reset_all_data\") == \"true\"\n",
    "DBDemos.setup_schema(catalog, db, reset_all_data, volume_name)\n",
    "folder = f\"/Volumes/{catalog}/{db}/{volume_name}\"\n",
    "\n",
    "data_exists = False\n",
    "try:\n",
    "  dbutils.fs.ls(folder)\n",
    "  dbutils.fs.ls(folder+\"/historical_turbine_status\")\n",
    "  dbutils.fs.ls(folder+\"/parts\")\n",
    "  dbutils.fs.ls(folder+\"/turbine\")\n",
    "  dbutils.fs.ls(folder+\"/incoming_data\")\n",
    "  data_exists = True\n",
    "  print(\"data already exists\")\n",
    "except Exception as e:\n",
    "  print(f\"folder doesn't exists, generating the data...\")\n",
    "\n",
    "\n",
    "def cleanup_folder(path):\n",
    "  #Cleanup to have something nicer\n",
    "  for f in dbutils.fs.ls(path):\n",
    "    if f.name.startswith('_committed') or f.name.startswith('_started') or f.name.startswith('_SUCCESS') :\n",
    "      dbutils.fs.rm(f.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "03c36656-c8ec-4ff9-8f6e-3983d1bef8bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_downloaded = True\n",
    "if not data_exists:\n",
    "    try:\n",
    "        DBDemos.download_file_from_git(folder+'/historical_turbine_status', \"databricks-demos\", \"dbdemos-dataset\", \"/manufacturing/lakehouse-iot-turbine/historical_turbine_status\")\n",
    "        DBDemos.download_file_from_git(folder+'/parts', \"databricks-demos\", \"dbdemos-dataset\", \"/manufacturing/lakehouse-iot-turbine/parts\")\n",
    "        DBDemos.download_file_from_git(folder+'/turbine', \"databricks-demos\", \"dbdemos-dataset\", \"/manufacturing/lakehouse-iot-turbine/turbine\")\n",
    "        DBDemos.download_file_from_git(folder+'/incoming_data', \"databricks-demos\", \"dbdemos-dataset\", \"/manufacturing/lakehouse-iot-turbine/incoming_data\")\n",
    "        spark.sql(\"CREATE TABLE IF NOT EXISTS turbine_power_prediction ( hour INT, min FLOAT, max FLOAT, prediction FLOAT);\")\n",
    "        spark.sql(\"insert into turbine_power_prediction values (0, 377, 397, 391), (1, 393, 423, 412), (2, 399, 455, 426), (3, 391, 445, 404), (4, 345, 394, 365), (5, 235, 340, 276), (6, 144, 275, 195), (7, 93, 175, 133), (8, 45, 105, 76), (9, 55, 125, 95), (10, 35, 99, 77), (11, 14, 79, 44)\")\n",
    "    except Exception as e: \n",
    "        data_downloaded = False\n",
    "        print(f\"Error trying to download the file from the repo: {str(e)}. Will generate the data instead...\")    \n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a6825d12-121a-4026-82d6-c6ee9a6aab5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#As we need a model in the DLT pipeline and the model depends of the DLT pipeline too, let's build an empty one.\n",
    "#This wouldn't make sense in a real-world system where we'd have 2 jobs / pipeline (1 for ingestion, and 1 to build the model / run inferences)\n",
    "import random\n",
    "import mlflow\n",
    "from  mlflow.models.signature import ModelSignature\n",
    "\n",
    "# define a custom model randomly flagging 10% of sensor for the demo init (it'll be replace with proper model on the training part.)\n",
    "class MaintenanceEmptyModel(mlflow.pyfunc.PythonModel):\n",
    "    def predict(self, context, model_input):\n",
    "        import random\n",
    "        sensors = ['sensor_F', 'sensor_D', 'sensor_B']  # List of sensors\n",
    "        return model_input['avg_energy'].apply(lambda x: 'ok' if random.random() < 0.9 else random.choice(sensors))\n",
    " \n",
    "#Enable Unity Catalog with mlflow registry\n",
    "mlflow.set_registry_uri('databricks-uc')\n",
    "model_name = \"dbdemos_turbine_maintenance\"\n",
    "#Only register empty model if model doesn't exist yet\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "try:\n",
    "  latest_model = client.get_model_version_by_alias(f\"{catalog}.{db}.{model_name}\", \"prod\")\n",
    "except Exception as e:\n",
    "    if \"RESOURCE_DOES_NOT_EXIST\" in str(e):\n",
    "        print(\"Model doesn't exist - saving an empty one\")\n",
    "        # setup the experiment folder\n",
    "        DBDemos.init_experiment_for_batch(\"lakehouse-iot-platform\", \"predictive_maintenance_mock\")\n",
    "        # save the model\n",
    "        churn_model = MaintenanceEmptyModel()\n",
    "        import pandas as pd\n",
    "\n",
    "        signature = ModelSignature.from_dict({'inputs': '[{\"name\": \"hourly_timestamp\", \"type\": \"datetime\"}, {\"name\": \"avg_energy\", \"type\": \"double\"}, {\"name\": \"std_sensor_A\", \"type\": \"double\"}, {\"name\": \"std_sensor_B\", \"type\": \"double\"}, {\"name\": \"std_sensor_C\", \"type\": \"double\"}, {\"name\": \"std_sensor_D\", \"type\": \"double\"}, {\"name\": \"std_sensor_E\", \"type\": \"double\"}, {\"name\": \"std_sensor_F\", \"type\": \"double\"}, {\"name\": \"percentiles_sensor_A\", \"type\": \"string\"}, {\"name\": \"percentiles_sensor_B\", \"type\": \"string\"}, {\"name\": \"percentiles_sensor_C\", \"type\": \"string\"}, {\"name\": \"percentiles_sensor_D\", \"type\": \"string\"}, {\"name\": \"percentiles_sensor_E\", \"type\": \"string\"}, {\"name\": \"percentiles_sensor_F\", \"type\": \"string\"}, {\"name\": \"location\", \"type\": \"string\"}, {\"name\": \"model\", \"type\": \"string\"}, {\"name\": \"state\", \"type\": \"string\"}]',\n",
    "'outputs': '[{\"type\": \"tensor\", \"tensor-spec\": {\"dtype\": \"object\", \"shape\": [-1]}}]'})\n",
    "        with mlflow.start_run() as run:\n",
    "            model_info = mlflow.pyfunc.log_model(artifact_path=\"model\", python_model=churn_model, signature=signature, pip_requirements=['scikit-learn==1.3.0', 'mlflow=='+mlflow.__version__])\n",
    "\n",
    "        #Register & move the model in production\n",
    "        model_registered = mlflow.register_model(f'runs:/{run.info.run_id}/model', f\"{catalog}.{db}.{model_name}\")\n",
    "        client.set_registered_model_alias(name=f\"{catalog}.{db}.{model_name}\", alias=\"prod\", version=model_registered.version)\n",
    "    else:\n",
    "        print(f\"ERROR: couldn't access model for unknown reason - DLT pipeline will likely fail as model isn't available: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "020c3208-4ef6-44a2-9057-3832e4444e47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if data_downloaded:\n",
    "    dbutils.notebook.exit(f\"Data Downloaded to {folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7aa50713-2846-41cd-9d42-6335609e8c58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mandrova.data_generator import SensorDataGenerator as sdg\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import uuid\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "\n",
    "def generate_sensor_data(turbine_id, sensor_conf, faulty = False, sample_size = 1000, display_graph = True, noise = 2, delta = -3):\n",
    "  dg = sdg()\n",
    "  rd = random.Random()\n",
    "  rd.seed(turbine_id)\n",
    "  dg.seed(turbine_id)\n",
    "  sigma = sensor_conf['sigma']\n",
    "  #Faulty, change the sigma with random value\n",
    "  if faulty:\n",
    "    sigma *= rd.randint(8,20)/10\n",
    "    \n",
    "  dg.generation_input.add_option(sensor_names=\"normal\", distribution=\"normal\", mu=0, sigma = sigma)\n",
    "  dg.generation_input.add_option(sensor_names=\"sin\", eq=f\"2*exp(sin(t))+{delta}\", initial={\"t\":0}, step={\"t\":sensor_conf['sin_step']})\n",
    "  dg.generate(sample_size)\n",
    "  sensor_name = \"sensor_\"+ sensor_conf['name']\n",
    "  dg.sum(sensors=[\"normal\", \"sin\"], save_to=sensor_name)\n",
    "  max_value = dg.data[sensor_name].max()\n",
    "  min_value = dg.data[sensor_name].min()\n",
    "  if faulty:\n",
    "    n_outliers = int(sample_size*0.15)\n",
    "    outliers = np.random.uniform(-max_value*rd.randint(2,3), max_value*rd.randint(2,3), n_outliers)\n",
    "    indicies = np.sort(np.random.randint(0, sample_size-1, n_outliers))\n",
    "    dg.inject(value=outliers, sensor=sensor_name, index=indicies)\n",
    "\n",
    "  n_outliers = int(sample_size*0.01)\n",
    "  outliers = np.random.uniform(min_value*noise, max_value*noise, n_outliers)\n",
    "  indicies = np.sort(np.random.randint(0, sample_size-1, n_outliers))\n",
    "  dg.inject(value=outliers, sensor=sensor_name, index=indicies)\n",
    "  \n",
    "  if display_graph:\n",
    "    dg.plot_data(sensors=[sensor_name])\n",
    "  return dg.data[sensor_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "50d8bce6-4d95-4525-9992-b8d89c11028a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sensors = [{\"name\": \"A\", \"sin_step\": 0, \"sigma\": 1},\n",
    "           {\"name\": \"B\", \"sin_step\": 0, \"sigma\": 2},\n",
    "           {\"name\": \"C\", \"sin_step\": 0, \"sigma\": 3},\n",
    "           {\"name\": \"D\", \"sin_step\": 0.1, \"sigma\": 1.5},\n",
    "           {\"name\": \"E\", \"sin_step\": 0.01, \"sigma\": 2},\n",
    "           {\"name\": \"F\", \"sin_step\": 0.2, \"sigma\": 1}]\n",
    "current_time = int(time.time()) - 3600*30\n",
    "\n",
    "#Sec between 2 metrics\n",
    "frequency_sec = 10\n",
    "#X points per turbine (1 point per frequency_sec second)\n",
    "sample_size = 2125\n",
    "turbine_count = 512\n",
    "dfs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e98ada4-34e2-44d1-9634-6484beba460b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def generate_turbine_data(turbine):\n",
    "  turbine = int(turbine)\n",
    "  rd = random.Random()\n",
    "  rd.seed(turbine)\n",
    "  damaged = turbine > turbine_count*0.6\n",
    "  if turbine % 10 == 0:\n",
    "    print(f\"generating turbine {turbine} - damage: {damaged}\")\n",
    "  df = pd.DataFrame()\n",
    "  damaged_sensors = []\n",
    "  rd.shuffle(sensors)\n",
    "  for s in sensors:\n",
    "    #30% change to have 1 sensor being damaged\n",
    "    #Only 1 sensor can send damaged metrics at a time to simplify the model. A C and E won't be damaged for simplification\n",
    "    if damaged and len(damaged_sensors) == 0 and s['name'] not in [\"A\", \"C\", \"E\"]:\n",
    "      damaged_sensor = rd.randint(1,10) > 5\n",
    "    else:\n",
    "      damaged_sensor = False\n",
    "    if damaged_sensor:\n",
    "      damaged_sensors.append('sensor_'+s['name'])\n",
    "    plot = turbine == 0\n",
    "    df['sensor_'+s['name']] = generate_sensor_data(turbine, s, damaged_sensor, sample_size, plot)\n",
    "\n",
    "  dg = sdg()\n",
    "  #Damaged turbine will produce less\n",
    "  factor = 50 if damaged else 30\n",
    "  energy = dg.generation_input.add_option(sensor_names=\"energy\", eq=\"x\", initial={\"x\":0}, step={\"x\":np.absolute(np.random.randn(sample_size).cumsum()/factor)})\n",
    "  dg.generate(sample_size, seed=rd.uniform(0,10000))\n",
    "  #Add some null values in some timeseries to get expectation metrics\n",
    "  if damaged and rd.randint(0,9) >7:\n",
    "    n_nulls = int(sample_size*0.005)\n",
    "    indicies = np.sort(np.random.randint(0, sample_size-1, n_nulls))\n",
    "    dg.inject(value=None, sensor=\"energy\", index=indicies)\n",
    "\n",
    "  if plot:\n",
    "    dg.plot_data()\n",
    "  df['energy'] = dg.data['energy']\n",
    "\n",
    "  df.insert(0, 'timestamp', range(current_time, current_time + len(df)*frequency_sec, frequency_sec))\n",
    "  df['turbine_id'] = str(uuid.UUID(int=rd.getrandbits(128)))\n",
    "  #df['damaged'] = damaged\n",
    "  df['abnormal_sensor'] = \"ok\" if len(damaged_sensors) == 0 else damaged_sensors[0]\n",
    "  return df\n",
    "\n",
    "from typing import Iterator\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import pandas_udf, col  \n",
    "\n",
    "df_schema=spark.createDataFrame(generate_turbine_data(0)) \n",
    "\n",
    "def generate_turbine(iterator: Iterator[pd.DataFrame]) -> Iterator[pd.DataFrame]:\n",
    "  for pdf in iterator:\n",
    "    for i, row in pdf.iterrows():\n",
    "      yield generate_turbine_data(row[\"id\"])\n",
    "\n",
    "spark_df = spark.range(0, turbine_count).repartition(int(turbine_count/10)).mapInPandas(generate_turbine, schema=df_schema.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b3f3c45-ceeb-465e-b419-aa2e863eb0ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "root_folder = folder\n",
    "folder_sensor = root_folder+'/incoming_data'\n",
    "spark_df.drop('damaged').drop('abnormal_sensor').orderBy('timestamp').repartition(100).write.mode('overwrite').format('parquet').save(folder_sensor)\n",
    "\n",
    "#Cleanup meta file to only keep names\n",
    "def cleanup(folder):\n",
    "  for f in dbutils.fs.ls(folder):\n",
    "    if not f.name.startswith('part-00'):\n",
    "      if not f.path.startswith('dbfs:/Volumes'):\n",
    "        raise Exception(f\"unexpected path, {f} throwing exception for safety\")\n",
    "      dbutils.fs.rm(f.path)\n",
    "      \n",
    "cleanup(folder_sensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0bbea74-1bf5-4804-98c7-5705ab155b7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from faker import Faker\n",
    "from pyspark.sql.types import ArrayType, FloatType, StringType\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "Faker.seed(0)\n",
    "faker = Faker()\n",
    "fake_latlng = F.udf(lambda: list(faker.local_latlng(country_code = 'US')), ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67a3eea0-947f-4c4e-894e-c059892d04d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rd = random.Random()\n",
    "rd.seed(0)\n",
    "folder = root_folder+'/turbine'\n",
    "(spark_df.select('turbine_id').drop_duplicates()\n",
    "   .withColumn('fake_lat_long', fake_latlng())\n",
    "   .withColumn('model', F.lit('EpicWind'))\n",
    "   .withColumn('lat', F.col('fake_lat_long').getItem(0))\n",
    "   .withColumn('long', F.col('fake_lat_long').getItem(1))\n",
    "   .withColumn('location', F.col('fake_lat_long').getItem(2))\n",
    "   .withColumn('country', F.col('fake_lat_long').getItem(3))\n",
    "   .withColumn('state', F.col('fake_lat_long').getItem(4))\n",
    "   .drop('fake_lat_long')\n",
    " .orderBy(F.rand()).repartition(1).write.mode('overwrite').format('json').save(folder))\n",
    "\n",
    "#Add some turbine with wrong data for expectations\n",
    "fake_null_uuid = F.udf(lambda: None if rd.randint(0,9) > 2 else str(uuid.uuid4()))\n",
    "df_error = (spark_df.select('turbine_id').limit(30)\n",
    "   .withColumn('turbine_id', fake_null_uuid())\n",
    "   .withColumn('fake_lat_long', fake_latlng())\n",
    "   .withColumn('model', F.lit('EpicWind'))\n",
    "   .withColumn('lat', F.lit(\"ERROR\"))\n",
    "   .withColumn('long', F.lit(\"ERROR\"))\n",
    "   .withColumn('location', F.col('fake_lat_long').getItem(2))\n",
    "   .withColumn('country', F.col('fake_lat_long').getItem(3))\n",
    "   .withColumn('state', F.col('fake_lat_long').getItem(4))\n",
    "   .drop('fake_lat_long').repartition(1).write.mode('append').format('json').save(folder))\n",
    "cleanup(folder)\n",
    "\n",
    "folder_status = root_folder+'/historical_turbine_status'\n",
    "(spark_df.select('turbine_id', 'abnormal_sensor').drop_duplicates()\n",
    "         .withColumn('start_time', (F.lit(current_time-1000)-F.rand()*2000).cast('int'))\n",
    "         .withColumn('end_time', (F.lit(current_time+3600*24*30)+F.rand()*4000).cast('int'))\n",
    "         .repartition(1).write.mode('overwrite').format('json').save(folder_status))\n",
    "cleanup(folder_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82744c65-5fa6-4eef-9c2f-7ae295452f49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/5/52/EERE_illust_large_turbine.gif\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39ff2ea5-fb0d-4a41-8b87-c399125c0c59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#see https://blog.enerpac.com/wind-turbine-maintenance-components-strategies-and-tools/\n",
    "#Get the list of states where our wind turbine are\n",
    "states = spark.read.json(folder).select('state').distinct().collect()\n",
    "states = [s['state'] for s in states]\n",
    "#For each state, we'll generate supply chain parts\n",
    "part_categories = [{'name': 'blade'}, {'name': 'Yaw drive'}, {'name': 'Brake'}, {'name': 'anemometer'}, {'name': 'controller card #1'}, {'name': 'controller card #2'}, {'name': 'Yaw motor'}, {'name': 'hydraulics'}, {'name': 'electronic guidance system'}]\n",
    "sensors = [c for c in spark.read.parquet(folder_sensor).columns if \"sensor\" in c]\n",
    "parts = []\n",
    "for p in part_categories:\n",
    "  for _ in range (1, rd.randint(30, 100)):\n",
    "    part = {}\n",
    "    part['EAN'] = None if rd.randint(0,100) > 95 else faker.ean(length=8)\n",
    "    part['type'] = p['name']\n",
    "    part['width'] = rd.randint(100,2000)\n",
    "    part['height'] = rd.randint(100,2000)\n",
    "    part['weight'] = rd.randint(100,20000)\n",
    "    part['stock_available'] = rd.randint(0, 5)\n",
    "    part['stock_location'] =   random.sample(states, 1)[0]\n",
    "    part['production_time'] = rd.randint(0, 5)\n",
    "    part['approvisioning_estimated_days'] = rd.randint(30,360)\n",
    "    part['sensors'] = random.sample(sensors, rd.randint(1,3))\n",
    "    parts.append(part)\n",
    "df = spark.createDataFrame(parts)\n",
    "folder_parts = root_folder+'/parts'\n",
    "df.repartition(3).write.mode('overwrite').format('json').save(folder_parts)\n",
    "cleanup(folder_parts)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "01-load-data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
